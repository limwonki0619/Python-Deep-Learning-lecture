{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [과제 26] 와인 등급 맞추기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import os\n",
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "numpy.random.seed(seed)\n",
    "tf.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.9968</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.9980</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2    3      4     5     6       7     8     9    10  11  12\n",
       "0   7.4  0.70  0.00  1.9  0.076  11.0  34.0  0.9978  3.51  0.56  9.4   5   1\n",
       "1   7.8  0.88  0.00  2.6  0.098  25.0  67.0  0.9968  3.20  0.68  9.8   5   1\n",
       "2   7.8  0.76  0.04  2.3  0.092  15.0  54.0  0.9970  3.26  0.65  9.8   5   1\n",
       "3  11.2  0.28  0.56  1.9  0.075  17.0  60.0  0.9980  3.16  0.58  9.8   6   1\n",
       "4   7.4  0.70  0.00  1.9  0.076  11.0  34.0  0.9978  3.51  0.56  9.4   5   1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../dataset/wine.csv\", header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df.values\n",
    "X = dataset[:, 0:11] # 12번 변수를 제와한 11개 변수 선택 \n",
    "Y = dataset[:, 11]\n",
    "Y_encoded = np_utils.to_categorical(Y)  # 원-핫 인코딩 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state=seed)\n",
    "\n",
    "Y_train_encoded = np_utils.to_categorical(Y_train)\n",
    "Y_test_encoded = np_utils.to_categorical(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4547, 11)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **[모델 1] : 64 x 32 x 16 x 10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = Sequential()\n",
    "model_1.add(Dense(64,  input_dim = 11, activation='relu'))\n",
    "model_1.add(Dense(32, activation='relu'))\n",
    "model_1.add(Dense(16, activation='relu'))\n",
    "model_1.add(Dense(10, activation='softmax'))  # 다중 분류 모델을 위한 소프트멕스 활성화 함수 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.compile(loss='categorical_crossentropy',  # 다중 분류 모델에 사용하는 로스 함수 \n",
    "              optimizer = 'adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 자동중단을 통해 최적의 모델 찾기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장 폴더 설장 \n",
    "MODEL_DIR = './model_1/'\n",
    "\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.mkdir(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 업데이트 및 저장 \n",
    "modelpath=\"./model_1/{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 자동 중단 설정 \n",
    "early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.09324, saving model to ./model_1/01-1.0932.hdf5\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.09324 to 1.06983, saving model to ./model_1/02-1.0698.hdf5\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.06983\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.06983\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.06983\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.06983\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.06983\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.06983\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.06983\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.06983\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.06983\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.06983\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.06983\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.06983\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.06983 to 1.05778, saving model to ./model_1/15-1.0578.hdf5\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.05778\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.05778\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.05778\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.05778\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.05778\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.05778\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.05778\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.05778\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.05778\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.05778\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.05778\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.05778\n",
      "\n",
      "Epoch 00028: val_loss improved from 1.05778 to 1.04745, saving model to ./model_1/28-1.0474.hdf5\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.04745\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.04745\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 1.04745\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 1.04745\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1.04745\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1.04745\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1.04745\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 1.04745\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 1.04745\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 1.04745\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 1.04745\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 1.04745\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 1.04745\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 1.04745\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 1.04745\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 1.04745\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 1.04745\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 1.04745\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 1.04745\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 1.04745\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 1.04745\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 1.04745\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 1.04745\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 1.04745\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 1.04745\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 1.04745\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 1.04745\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 1.04745\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 1.04745\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 1.04745\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 1.04745\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 1.04745\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 1.04745\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 1.04745\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 1.04745\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 1.04745\n",
      "\n",
      "Epoch 00065: val_loss improved from 1.04745 to 1.03863, saving model to ./model_1/65-1.0386.hdf5\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00128: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 1.03863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00156: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00201: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00202: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00203: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00204: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00205: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00206: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00207: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00208: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00209: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00210: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00211: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00212: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00213: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00214: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00215: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00216: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00217: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00218: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00219: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00220: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00221: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00222: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00223: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00224: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00225: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00226: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00227: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00228: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00229: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00230: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00231: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00232: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00233: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00234: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00235: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00236: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00237: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00238: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00239: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00240: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00241: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00242: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00243: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00244: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00245: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00246: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00247: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00248: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00249: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00250: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00251: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00252: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00253: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00254: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00255: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00256: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00257: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00258: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00259: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00260: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00261: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00262: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00263: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00264: val_loss did not improve from 1.03863\n",
      "\n",
      "Epoch 00265: val_loss did not improve from 1.03863\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x4c920dd8>"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.fit(X, Y_encoded, validation_split=0.3, epochs=2000, batch_size=100, verbose=0,\n",
    "           callbacks=[early_stopping_callback, checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6497/6497 [==============================] - 1s 103us/step\n",
      "\n",
      " Test Accuracy: 0.5447\n"
     ]
    }
   ],
   "source": [
    "del model_1 \n",
    "model_1 = load_model('model_1/65-1.0386.hdf5')\n",
    "print(\"\\n Test Accuracy: %.4f\" % (model_1.evaluate(X, Y_encoded)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [모델 1]의 테스트 셋  정확도 0.5447"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **[모델 2] 256 x 128 x 64 x 32 x 16 x 10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = Sequential()\n",
    "\n",
    "model_2.add(Dense(256,  input_dim = 11, activation='relu'))\n",
    "model_2.add(Dense(128, activation='relu'))\n",
    "model_2.add(Dense(64, activation='relu'))\n",
    "model_2.add(Dense(32, activation='relu'))\n",
    "model_2.add(Dense(16, activation='relu'))\n",
    "model_2.add(Dense(10, activation='softmax'))  # 다중 분류 모델을 위한 소프트멕스 활성화 함수 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.compile(loss='categorical_crossentropy',  # 다중 분류 모델에 사용하는 로스 함수 \n",
    "              optimizer = 'adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장 폴더 설장 \n",
    "MODEL_DIR = './model_2/'\n",
    "\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.mkdir(MODEL_DIR)\n",
    "    \n",
    "# 모델 업데이트 및 저장 \n",
    "modelpath=\"./model_2/{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "\n",
    "# 학습 자동 중단 설정 \n",
    "early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss did not improve from 1.53684\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.53684\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.53684\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.53684\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.53684\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.53684\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.53684\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.53684\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.53684\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.53684\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.53684\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.53684\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.53684 to 1.53592, saving model to ./model_2/13-1.5359.hdf5\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00106: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 1.53592\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 1.53592\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x54697748>"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.fit(X, Y_encoded, validation_split=0.3, epochs=2000, batch_size=200, verbose=0,\n",
    "           callbacks=[early_stopping_callback, checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6497/6497 [==============================] - 1s 126us/step\n",
      "\n",
      " Test Accuracy: 0.6137\n"
     ]
    }
   ],
   "source": [
    "del model_2 \n",
    "model_2 = load_model('model_2/13-1.5359.hdf5')\n",
    "print(\"\\n Test Accuracy: %.4f\" % (model_2.evaluate(X, Y_encoded)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [모델 2]의 예측 정확도 0.6137\n",
    "- [모델 2]에서 은닉층을 더 많이 생성한 결과 [모델 1]의 정확도(0.5447)보다 0.069%p 증가한 것으로 나타났다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **[모델 3] 55 x 44 x 33 x 22 x 10**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = Sequential()\n",
    "\n",
    "model_3.add(Dense(11, input_dim = 11, activation='relu'))\n",
    "model_3.add(Dense(64, activation='relu'))\n",
    "model_3.add(Dense(32, activation='relu'))\n",
    "model_3.add(Dense(10, activation='softmax'))  # 다중 분류 모델을 위한 소프트멕스 활성화 함수 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.compile(loss='categorical_crossentropy',  # 다중 분류 모델에 사용하는 로스 함수 \n",
    "              optimizer = 'adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장 폴더 설장 \n",
    "MODEL_DIR = './model_3/'\n",
    "\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.mkdir(MODEL_DIR)\n",
    "    \n",
    "# 모델 업데이트 및 저장 \n",
    "modelpath=\"./model_3/{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "\n",
    "# 학습 자동 중단 설정 \n",
    "early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.06097, saving model to ./model_3/01-1.0610.hdf5\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 1.06097\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 1.06097\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x5f429470>"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3.fit(X, Y_encoded, validation_split=0.3, epochs=2000, batch_size=200, verbose=0,\n",
    "           callbacks=[early_stopping_callback, checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6497/6497 [==============================] - 1s 142us/step\n",
      "\n",
      " Test Accuracy: 0.5446\n"
     ]
    }
   ],
   "source": [
    "del model_3 \n",
    "model_3 = load_model('model_3/01-1.0610.hdf5')\n",
    "print(\"\\n Test Accuracy: %.4f\" % (model_3.evaluate(X, Y_encoded)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **[모델 4]회귀식으로 풀어보기 64 x 32 x 16 x 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4 = Sequential()\n",
    "\n",
    "model_4.add(Dense(64, input_dim = 11, activation='relu'))\n",
    "model_4.add(Dense(32, activation='relu'))\n",
    "model_4.add(Dense(16, activation='relu'))\n",
    "model_4.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장 폴더 설장 \n",
    "MODEL_DIR = './model_4/'\n",
    "\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.mkdir(MODEL_DIR)\n",
    "    \n",
    "# 모델 업데이트 및 저장 \n",
    "modelpath=\"./model_4/{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
    "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "\n",
    "# 학습 자동 중단 설정 \n",
    "early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.46961, saving model to ./model_4/01-0.4696.hdf5\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.46961\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.46961\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.46961 to 0.46475, saving model to ./model_4/04-0.4647.hdf5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.46475 to 0.46097, saving model to ./model_4/05-0.4610.hdf5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 0.46097\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.46097\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x66f4f940>"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_4.fit(X_train, Y_train, validation_split=0.3, epochs=2000, batch_size=10, verbose=0,\n",
    "           callbacks=[early_stopping_callback, checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model_4 \n",
    "model_4 = load_model('model_4/05-0.4610.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 값과 실제 값의 비교\n",
    "\n",
    "Y_prediction = model_4.predict(X).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실제 등급 : 6.0, 예상 등급 : 5.0\n",
      "실제 등급 : 6.0, 예상 등급 : 5.0\n",
      "실제 등급 : 6.0, 예상 등급 : 5.0\n",
      "실제 등급 : 5.0, 예상 등급 : 5.0\n",
      "실제 등급 : 8.0, 예상 등급 : 5.0\n",
      "실제 등급 : 5.0, 예상 등급 : 5.0\n",
      "실제 등급 : 6.0, 예상 등급 : 5.0\n",
      "실제 등급 : 6.0, 예상 등급 : 5.0\n",
      "실제 등급 : 6.0, 예상 등급 : 5.0\n",
      "실제 등급 : 6.0, 예상 등급 : 6.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    real = Y_test[i]\n",
    "    prediction = Y_prediction[i]\n",
    "    print(\"실제 등급 : {0}, 예상 등급 : {1:.4}\".format(real, round(prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6497/6497 [==============================] - 0s 34us/step\n",
      "\n",
      " Test Accuracy: 0.4896\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Test Accuracy: %.4f\" % (model_4.evaluate(X, Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 회귀식을 사용해 예측을 한 [모델 4]의 경우 예측값이 0.4896으로 [모델 2]에 비해 많이 감소하였다.\n",
    "- 앞에서 모든 변수를 사용해서 예측을 했지만, 높은 수준의 예측률이 나타나지 않았으며 가장 높은 예측 모델은 약 61%의 예축률을 보였다.\n",
    "- 따라서 더 나은 모델을 만들기 위해 와인의 등급에 양의 영향을 주는 변수를 몇 개 선택해 분석하는 방법이 필요(?)하다고 느낀다.. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
